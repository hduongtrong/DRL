{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Review of Markov Decision Processes\n",
    "\n",
    "**Note to students on how this assignment is organized.**\n",
    "Your answers should be written in a file called `hw1.py` that is located in the same directory as this file. (OR, it can be in a different directory, but the directory should be on your `PYTHONPATH`, so `import hw1` succeeds.)\n",
    "\n",
    "If you look at `hw1_template.py`, you can see all of the functions you have to write and values you have to set. You won't have to write modify this ipython notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This assignment will review exact methods for solving Markov Decision Processes (MDPs) with finite state and action spaces.\n",
    "We will implement policy iteration (PI) and value iteration (VI) for a finite MDP, both of which find the optimal policy in a finite number of iterations.\n",
    "\n",
    "For this assignment, we will consider discounted infinite-horizon MDPs. Here, the MDP is defined by the tuple $(S, A, R, P, \\gamma)$, where\n",
    "\n",
    "- S: state space (set)\n",
    "- A: action space (set)\n",
    "- R(s,a,s'): reward function, $S \\times A \\times S \\rightarrow \\mathbb{R}$, where $s$ is current state and $s'$ is next state \n",
    "- P(s,a,s'): transition probability kernel $p(s' | s, a)$, $S \\times A \\times S \\rightarrow \\mathbb{R}$\n",
    "- $\\gamma$: discount $\\in (0,1)$\n",
    "\n",
    "Here we will consider MDPs where $S,A$ are finite sets, hence $R$ and $P$ are 3D arrays\n",
    "\n",
    "Next, we'll randomly generate an MDP which your algorithms should be able to solve.\n",
    "Using randomly generated MDPs might seem a bit dry (don't worry, we'll look at some exciting ones later!), but it emphasizes that policy iteration and value iteration just boil down to a few operations on arrays, when we have finite state and action spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# == Notebook setup ==\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "INSTRUCTOR=True # this flag switches between instructor mode and student mode\n",
    "\n",
    "import numpy as np, numpy.random as nr\n",
    "np.random.seed(0)\n",
    "\n",
    "import hw_utils\n",
    "import hw1 # YOUR ANSWERS HERE\n",
    "# ===================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Value Iteration\n",
    "\n",
    "First, let's randomly generate the reward function and transition probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nS_rand = 10 # number of states\n",
    "nA_rand = 2 # number of actions\n",
    "R_rand = nr.rand(nS_rand, nA_rand, nS_rand) # reward function\n",
    "# R[i,j,k] := R(s=i, a=j, s'=k), \n",
    "# i.e., the dimensions are (current state, action, next state)\n",
    "P_rand = nr.rand(nS_rand, nA_rand, nS_rand) \n",
    "# P[i,j,k] := P(s'=k | s=i, a=j)\n",
    "# i.e., dimensions are (current state, action, next state)\n",
    "\n",
    "# BE CAREFUL THAT YOU DON'T MIX UP THE 0TH AND 2ND DIMENISION OF R AND T!\n",
    "# REMEMBER THAT THE AXES CORRESPOND TO S,A,S', NOT S',A,S\n",
    "P_rand /= P_rand.sum(axis=2,keepdims=True) # normalize conditional probabilities\n",
    "gamma = 0.90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1a: implement value iteration update\n",
    "Your task is to write a function called `vstar_backup`, taking the following arguments:\n",
    "\n",
    "- Vprev: value function from previous iteration $V^{(n)}$\n",
    "- T: transition function\n",
    "- R: reward function\n",
    "- gamma: discount factor.\n",
    "\n",
    "It should return the following arrays:\n",
    "\n",
    "- V: new value function $V^{(n+1)}$, defined as follows:\n",
    "\n",
    "$$V^{(n+1)}(s) = \\max_a \\sum_{s'} P(s,a,s') [ R(s,a,s') + \\gamma V^{(n)}(s')]$$\n",
    "\n",
    "- $A^{(n+1)}$: the actions that are greedy with respect to $V^{(n)}$, i.e.,\n",
    "\n",
    "$$V^{(n+1)}(s) = \\operatorname*{argmax}_a \\sum_{s'} P(s,a,s') [ R(s,a,s') + \\gamma V^{(n)}(s')]$$\n",
    "\n",
    "\n",
    "This update is often called a **backup**, since we are updating the state $s$ based on possible  future states $s'$, i.e., we are propagating the value function *backwards in time*. \n",
    "The function is called **vstar**_backup because the fixed point of this update is the optimal value function $V^*$.\n",
    "\n",
    "Your function will be called by the value iteration loop, provided below.\n",
    "Your function's output should identically match the provided output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         iter |  max|V-Vprev| | # chg actions |          V[0]\n",
      "-------------------------------------------------------------\n",
      "            1 |      0.707147 |           N/A |      0.618258\n",
      "            2 |      0.514599 |             1 |       1.13286\n",
      "            3 |      0.452404 |             0 |       1.58322\n",
      "            4 |      0.405723 |             0 |       1.98855\n",
      "            5 |      0.364829 |             0 |       2.35327\n",
      "            6 |      0.328307 |             0 |       2.68157\n",
      "            7 |      0.295474 |             0 |       2.97704\n",
      "            8 |      0.265926 |             0 |       3.24297\n",
      "            9 |      0.239333 |             0 |        3.4823\n",
      "           10 |        0.2154 |             0 |        3.6977\n",
      "           11 |       0.19386 |             0 |       3.89156\n",
      "           12 |      0.174474 |             0 |       4.06604\n",
      "           13 |      0.157026 |             0 |       4.22306\n",
      "           14 |      0.141324 |             0 |       4.36439\n",
      "           15 |      0.127191 |             0 |       4.49158\n",
      "           16 |      0.114472 |             0 |       4.60605\n",
      "           17 |      0.103025 |             0 |       4.70908\n",
      "           18 |     0.0927225 |             0 |        4.8018\n",
      "           19 |     0.0834503 |             0 |       4.88525\n",
      "           20 |     0.0751053 |             0 |       4.96035\n"
     ]
    }
   ],
   "source": [
    "def value_iteration(T,R,gamma, n_iter):\n",
    "    \"\"\"\n",
    "    Run `n_iter` iterations of value iteration, where T,R,gamma are the\n",
    "    transition probabilities, reward functions, and discount factor\n",
    "    of an MDP.\n",
    "    \"\"\"\n",
    "    nS = T.shape[0]\n",
    "    Vprev = np.zeros(nS)\n",
    "    Aprev = None\n",
    "    print hw_utils.fmt_row(13, [\"iter\", \"max|V-Vprev|\", \"# chg actions\", \"V[0]\"], header=True)\n",
    "    for i in xrange(n_iter):\n",
    "        V,A = hw1.vstar_backup(Vprev, T, R, gamma)    \n",
    "        print hw_utils.fmt_row(13, [i+1, np.abs(V-Vprev).max(), \"N/A\" if Aprev is None else (A != Aprev).sum(), V[0]])\n",
    "        Vprev,Aprev = V,A\n",
    "        \n",
    "value_iteration(T_rand,R_rand,gamma, n_iter=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that value iteration only takes two iterations to converge to the right actions everywhere. (However, note that the actual values converge rather slowly.) Think about why that might be the case for MDPs with transition probabilities generated as above.\n",
    "Also, note that the value of any particular state (e.g., V[0], shown in the rightmost column) increases monotonically. Under which conditions that is true? [question will not be graded]\n",
    "\n",
    "### Problem 1b: create an MDP such for which value iteration takes a long time to converge\n",
    "Specifically, the requirement is that your MDP should have 10 states and 2 actions, and at least one element of $A^{(n)}$ should change each iteration (i.e., `#change actions` in the displayed table should be nonzero for at least iterations 2 through 9).\n",
    "\n",
    "Here's a hint for one solution: arrange the states a line, so that state i can only transition to one of {i-1, i, i+1}.\n",
    "You should create 3D arrays T,R in `hw1.py` that define the MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         iter |  max|V-Vprev| | # chg actions |          V[0]\n",
      "-------------------------------------------------------------\n",
      "            1 |             1 |           N/A |             0\n",
      "            2 |           0.9 |             2 |             0\n",
      "            3 |          0.81 |             1 |             0\n",
      "            4 |         0.729 |             1 |             0\n",
      "            5 |        0.6561 |             1 |             0\n",
      "            6 |       0.59049 |             1 |             0\n",
      "            7 |      0.531441 |             1 |             0\n",
      "            8 |      0.478297 |             1 |             0\n",
      "            9 |      0.430467 |             1 |             0\n",
      "           10 |       0.38742 |             1 |       0.38742\n",
      "           11 |      0.348678 |             0 |      0.736099\n",
      "           12 |      0.313811 |             0 |       1.04991\n",
      "           13 |       0.28243 |             0 |       1.33234\n",
      "           14 |      0.254187 |             0 |       1.58653\n",
      "           15 |      0.228768 |             0 |       1.81529\n",
      "           16 |      0.205891 |             0 |       2.02118\n",
      "           17 |      0.185302 |             0 |       2.20649\n",
      "           18 |      0.166772 |             0 |       2.37326\n",
      "           19 |      0.150095 |             0 |       2.52335\n",
      "           20 |      0.135085 |             0 |       2.65844\n"
     ]
    }
   ],
   "source": [
    "nS = 10\n",
    "assert hw1.P.shape == (10,2,10), \"T has the wrong shape\"\n",
    "assert hw1.R.shape == (10,2,10), \"R has the wrong shape\"\n",
    "assert np.allclose(hw1.P.sum(axis=2), np.ones((10,2))), \"Transition probabilities should sum to 1\"\n",
    "\n",
    "value_iteration(hw1.P, hw1.R, gamma, n_iter=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Policy Iteration\n",
    "\n",
    "The next task is to implement exact policy iteration (PI).\n",
    "\n",
    "PI first initializes the policy $\\pi_0(s)$, and then it performs the following two steps on the $n$th iteration, for $n=1,2,...$:\n",
    "1. Compute state-action value function $Q^{\\pi_{n-1}}(s,a)$ of policy $\\pi_{n-1}$\n",
    "2. Compute new policy $\\pi_n(s) = \\operatorname*{argmax}_a Q^{\\pi_{n-1}}(s,a)$\n",
    "\n",
    "We'll break step 1 into two parts.\n",
    "\n",
    "### Problem 2a: state value function\n",
    "\n",
    "First you'll write a function called `compute_vpi` that computes the state-value function $V^{\\pi}$ for an arbitrary policy $\\pi$.\n",
    "Recall that $V^{\\pi}$ satisfies the following linear equation:\n",
    "$$V^{\\pi}(s) = \\sum_{s'} T(s,\\pi(s),s')[ R(s,\\pi(s),s') + \\gamma V^{\\pi}(s')]$$\n",
    "\n",
    "Your function should take as inputs:\n",
    "\n",
    "- pi: 1D array specifying action at every state (i.e., the policy)\n",
    "- T: transition probabilities\n",
    "- R: reward function\n",
    "- gamma: discount\n",
    "\n",
    "Your function should output:\n",
    "\n",
    "- Vpi: 1D array specifying value function at each state, satisfying the Bellman equation above.\n",
    "\n",
    "Your function's output should exactly match the provided output below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.206217    5.15900351  5.01725926  4.76913715  5.03154609  5.06171323\n",
      "  4.97964471  5.28555573  5.13320501  5.08988046]\n"
     ]
    }
   ],
   "source": [
    "pi0 = np.zeros(nS,dtype='i') # policy performs action 0 at every state\n",
    "Vpi = hw1.compute_vpi(pi0, P_rand, R_rand, gamma)\n",
    "print Vpi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2b: state-action value function\n",
    "\n",
    "Next, you'll write a function to compute the state-action value function $Q^{\\pi}$, defined as follows\n",
    "$$Q^{\\pi}(s) = \\sum_{s'} T(s,a,s')[ R(s,a,s') + \\gamma V^{\\pi}(s')]$$\n",
    "\n",
    "Your function should take as inputs:\n",
    "\n",
    "- pi: 1D array specifying action at every state (i.e., the policy)\n",
    "- T: transition probabilities\n",
    "- R: reward function\n",
    "- gamma: discount\n",
    "\n",
    "Your function should output: \n",
    "\n",
    "- Qpi, a 2d array with axes corresponding to (state, action), satisfying the equation above.\n",
    "\n",
    "Your function's output should exactly match the output below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.206217  ,  5.20238706],\n",
       "       [ 5.15900351,  5.1664316 ],\n",
       "       [ 5.01725926,  4.99211906],\n",
       "       [ 4.76913715,  4.98080235],\n",
       "       [ 5.03154609,  4.89448888],\n",
       "       [ 5.06171323,  5.29418621],\n",
       "       [ 4.97964471,  5.06868986],\n",
       "       [ 5.28555573,  4.9156956 ],\n",
       "       [ 5.13320501,  4.97736801],\n",
       "       [ 5.08988046,  5.00511597]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hw1.compute_qpi(pi0, P_rand, R_rand, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code runs the policy iteration algorithm. Your output should match the provided output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         iter | # chg actions |        Q[0,0]\n",
      "---------------------------------------------\n",
      "            1 |             4 |       5.20622\n",
      "            2 |             2 |       5.59042\n",
      "            3 |             0 |        5.6255\n",
      "            4 |             0 |        5.6255\n",
      "            5 |             0 |        5.6255\n",
      "            6 |             0 |        5.6255\n",
      "            7 |             0 |        5.6255\n",
      "            8 |             0 |        5.6255\n",
      "            9 |             0 |        5.6255\n",
      "           10 |             0 |        5.6255\n"
     ]
    }
   ],
   "source": [
    "def policy_iteration(P, R, gamma, n_iter):\n",
    "    pi_prev = np.zeros(P.shape[0],dtype='i')\n",
    "    \n",
    "    print hw_utils.fmt_row(13, [\"iter\", \"# chg actions\", \"Q[0,0]\"], header=True)\n",
    "    \n",
    "    for i in xrange(n_iter):\n",
    "        qpi = hw1.compute_qpi(pi_prev, P, R, gamma)\n",
    "        pi = qpi.argmax(axis=1)\n",
    "        print hw_utils.fmt_row(13, [i+1, (pi != pi_prev).sum(),  qpi[0,0]])\n",
    "        pi_prev = pi\n",
    "        \n",
    "policy_iteration(T_rand, R_rand, gamma, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
